=== Packing Model Conversion Task ===
Source:  llava_vit_large_ln
Target:  hf_llava_vit_packing_large_ln
Weights: /video_vit/xiangan/checkpoint_llava_vit/2025_11_22_new_l14_continue_128gpus_how_to_100m_448px_224px/00148000/backbone.pt
Output:  test

--> Creating Source Model...
--> Loading weights into Source...

--> Creating Packing Target Model...
--> Remapping State Dict for Packing Model...
[Remap Packing] Starting state dict remapping for packing model...
--> Loading weights into Packing Target...
    Load OK (No critical missing keys).

--> [CUDA DETECTED] Moving models to cuda and casting to bfloat16...

--> Fetching real image for verification (448x448)...
--> Downloading real image from http://images.cocodataset.org/val2017/000000039769.jpg (Target Size: 448)...

=== Verifying Consistency with Packing Model (grid_thw input - bfloat16) ===
    Running on Device: cuda:0
    Model Dtype: torch.bfloat16
    Input Shape: torch.Size([1, 3, 448, 448]) | Dtype: torch.bfloat16
    grid_thw: tensor([[ 1, 32, 32]], device='cuda:0')
    Packing input shape: torch.Size([1024, 588]) (seq_len=1024, patch_dim=588)
    patch_positions shape: torch.Size([1024, 3])
    [Packing Feature] Max Diff:       1.332031
    [Packing Feature] Min Cosine Sim: 0.84566748 (Mean: 0.99936903)
    ❌ Packing Feature: FAIL
    [Packing Head]    Max Diff:       0.007812
    [Packing Head]    Min Cosine Sim: 0.99997479 (Mean: 0.99997479)
    ✅ Packing Head:    PASS

=== Verifying Video Consistency with Packing Model (8 frames - bfloat16) ===
    Running on Device: cuda:0
    Model Dtype: torch.bfloat16
    Video Input Shape: torch.Size([1, 3, 8, 224, 224]) (B, C, T, H, W)
    Original frame indices: [0, 1, 2, 3, 4, 5, 6, 7]
    Interpolated indices (in 64-frame context): [0, 9, 18, 27, 36, 45, 54, 63]
    Padded video shape: torch.Size([1, 3, 64, 224, 224])
    visible_index shape: torch.Size([1, 2048])
    Packing input shape: torch.Size([2048, 588]) (seq_len=2048, patch_dim=588)
    patch_positions shape: torch.Size([2048, 3])
    grid_thw: [[8, 16, 16]]
    [Video Packing Feature] Max Diff:       3.664062
    [Video Packing Feature] Min Cosine Sim: -0.14894648 (Mean: 0.99261725)
    ❌ Video Packing Feature: FAIL
    [Video Packing Head]    Max Diff:       0.011719
    [Video Packing Head]    Min Cosine Sim: 0.99996501 (Mean: 0.99996501)
    ✅ Video Packing Head:    PASS

=== Verifying Mixed Video+Image Consistency with Packing Model (8 frames + image - bfloat16) ===
    Running on Device: cuda:0
    Model Dtype: torch.bfloat16
    Video Input Shape: torch.Size([1, 3, 8, 224, 224]) (B, C, T, H, W)
    Video original frame indices: [0, 1, 2, 3, 4, 5, 6, 7]
    Video interpolated indices (in 64-frame context): [0, 9, 18, 27, 36, 45, 54, 63]
    Image Input Shape: torch.Size([1, 3, 448, 448]) (B, C, H, W)
    Video padded shape: torch.Size([1, 3, 64, 224, 224])
    Video visible_index shape: torch.Size([1, 2048])
    Source image output shape: torch.Size([1, 1024, 1024])
    Combined input shape: torch.Size([3072, 588])
    Combined patch_positions shape: torch.Size([3072, 3])
    Combined grid_thw: [[8, 16, 16], [1, 32, 32]]
    Packing video output shape: torch.Size([2048, 1024])
    Packing image output shape: torch.Size([1024, 1024])

    --- Video Comparison ---
    [Mixed Video Feature] Max Diff:       3.664062
    [Mixed Video Feature] Min Cosine Sim: -0.14894648 (Mean: 0.99261725)
    ❌ Mixed Video Feature: FAIL

    --- Image Comparison ---
    [Mixed Image Feature] Max Diff:       0.351562
    [Mixed Image Feature] Min Cosine Sim: 0.95027447 (Mean: 0.99950600)
    ❌ Mixed Image Feature: FAIL

    --- Mixed Video+Image Overall Summary ---
    ❌ Mixed Video+Image Consistency: SOME FAILED

=== Verifying Multi-Sample Consistency (3 images + 2 videos - bfloat16) ===
    Image resolutions: 224, 336, 1080
    Video resolutions: 378 (8 frames), 518 (8 frames)
    Running on Device: cuda:0
    Model Dtype: torch.bfloat16
    Original image sizes: [224, 336, 1008]
    Original video sizes: [378, 518]
    Adjusted image sizes (divisible by 14): [224, 336, 1008]
    Adjusted video sizes (divisible by 14): [378, 518]
    Image 1 Shape: torch.Size([1, 3, 224, 224]) (res=224, adjusted=224)
    Image 2 Shape: torch.Size([1, 3, 336, 336]) (res=336, adjusted=336)
    Image 3 Shape: torch.Size([1, 3, 1008, 1008]) (res=1008, adjusted=1008)
    Video 1 Shape: torch.Size([1, 3, 8, 378, 378]) (res=378, adjusted=378)
    Video 2 Shape: torch.Size([1, 3, 8, 518, 518]) (res=518, adjusted=518)
    Source image 1 output shape: torch.Size([1, 256, 1024])
    Source image 2 output shape: torch.Size([1, 576, 1024])
    Source image 3 output shape: torch.Size([1, 5184, 1024])
    Source video 1 output shape: torch.Size([1, 5832, 1024])
    Source video 2 output shape: torch.Size([1, 10952, 1024])
    Combined input shape: torch.Size([22800, 588])
    Combined patch_positions shape: torch.Size([22800, 3])
    Combined grid_thw: [[1, 16, 16], [1, 24, 24], [1, 72, 72], [8, 27, 27], [8, 37, 37]]
    Packing image 1 output shape: torch.Size([256, 1024])
    Packing image 2 output shape: torch.Size([576, 1024])
    Packing image 3 output shape: torch.Size([5184, 1024])
    Packing video 1 output shape: torch.Size([5832, 1024])
    Packing video 2 output shape: torch.Size([10952, 1024])

    --- Image 1 Comparison (res=224) ---
    [Image 1] Max Diff:       2.222656
    [Image 1] Min Cosine Sim: 0.56475443 (Mean: 0.99659854)
    ❌ Image 1: FAIL

    --- Image 2 Comparison (res=336) ---
    [Image 2] Max Diff:       0.210938
    [Image 2] Min Cosine Sim: 0.98896676 (Mean: 0.99956787)
    ❌ Image 2: FAIL

    --- Image 3 Comparison (res=1008) ---
    [Image 3] Max Diff:       4.140625
    [Image 3] Min Cosine Sim: -0.14622325 (Mean: 0.99286079)
    ❌ Image 3: FAIL

    --- Video 1 Comparison (res=378) ---
    [Video 1] Max Diff:       2.566406
    [Video 1] Min Cosine Sim: -0.05048360 (Mean: 0.99751592)
    ❌ Video 1: FAIL

    --- Video 2 Comparison (res=518) ---
    [Video 2] Max Diff:       2.791016
    [Video 2] Min Cosine Sim: -0.06705524 (Mean: 0.99796063)
    ❌ Video 2: FAIL

    --- Multi-Sample Overall Summary ---
    ❌ Multi-Sample Consistency (3 images + 2 videos): SOME FAILED

--> Saving HF Packing Model to test...
    Saving CLIPImageProcessor config (CLIP Defaults + 448)...
✅ Packing Model (bf16) and CLIP Processor saved.

=== Verifying Loaded Saved Packing Model (Simulating User Usage - bfloat16) ===
--> Loading from: test
    Using device from src_model: cuda:0
    Loading Image Processor (CLIP)...
    Processor config: CLIPImageProcessor {
  "crop_size": {
    "height": 448,
    "width": 448
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 448
  }
}

    Loading Vision Tower (LlavaViTPackingModel) with torch_dtype=bfloat16...
    ✅ Successfully loaded and moved to device.

    --- Image Test (Single Frame) ---
    Image grid_thw: [[1, 32, 32]]
    patch_positions shape: torch.Size([1024, 3])
    [Reloaded Image] Min Cosine Sim: 0.91173929 (Mean: 0.99934292)
    ❌ Reloaded Image Verification: FAIL

    --- Video Test (8 Frames) ---
    Video shape: torch.Size([1, 3, 8, 224, 224]) (B, C, T, H, W)
    Original frame indices: [0, 1, 2, 3, 4, 5, 6, 7]
    Interpolated indices (in 64-frame context): [0, 9, 18, 27, 36, 45, 54, 63]
    Packing input shape: torch.Size([2048, 588])
    patch_positions shape: torch.Size([2048, 3])
    grid_thw: [[8, 16, 16]]
    [Reloaded Video] Min Cosine Sim: -0.03206185 (Mean: 0.99299443)
    ❌ Reloaded Video Verification: FAIL

    --- Overall Summary ---
    ❌ Reloaded Packing Model (Image + Video): SOME FAILED

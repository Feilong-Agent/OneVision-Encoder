# LlavaViT ä¸Šä¼ åˆ° HuggingFace å¿«é€ŸæŒ‡å—

## ä¸€é”®ä¸Šä¼ 

```bash
# åŸºç¡€å‘½ä»¤
python model_factory/upload_llava_vit_to_hf.py \
    --model_name hf_llava_vit_large_ln \
    --weight_path /path/to/checkpoint.pth \
    --repo_id your-username/llava-vit-large \
    --token YOUR_HF_TOKEN
```

## ä¸Šä¼ åä½¿ç”¨

```python
from transformers import AutoModel

# åŠ è½½æ¨¡å‹ï¼ˆéœ€è¦è®¾ç½® trust_remote_code=Trueï¼‰
model = AutoModel.from_pretrained(
    "your-username/llava-vit-large", 
    trust_remote_code=True
)

# ä½¿ç”¨æ¨¡å‹
import torch
pixel_values = torch.randn(1, 3, 448, 448)
outputs = model(pixel_values=pixel_values)
```

## æ”¯æŒçš„æ¨¡å‹

- `hf_llava_vit_small_ln` - å°å‹ (384 hidden, 6 layers)
- `hf_llava_vit_base_ln` - åŸºç¡€ (768 hidden, 12 layers)
- `hf_llava_vit_large_ln` - å¤§å‹ (1024 hidden, 24 layers)
- `hf_llava_vit_huge_ln` - è¶…å¤§å‹ (1536 hidden, 27 layers)
- `hf_llava_vit_giant_ln` - å·¨å‹ (1536 hidden, 40 layers)

## æ ¸å¿ƒåŠŸèƒ½

ä¸Šä¼ è„šæœ¬ä¼šè‡ªåŠ¨ï¼š

1. âœ… é…ç½® `auto_map` - è®© AutoModel èƒ½è¯†åˆ«ä½ çš„æ¨¡å‹
2. âœ… åˆ›å»º `configuration_llava_vit.py` - ç‹¬ç«‹çš„é…ç½®ç±»æ–‡ä»¶
3. âœ… åˆ›å»º `modeling_llava_vit.py` - ç‹¬ç«‹çš„æ¨¡å‹ç±»æ–‡ä»¶
4. âœ… ä¿å­˜å›¾åƒå¤„ç†å™¨é…ç½® - CLIP é¢„å¤„ç†å‚æ•°
5. âœ… ç”Ÿæˆæ¨¡å‹å¡ç‰‡ (README.md) - è¯¦ç»†çš„ä½¿ç”¨è¯´æ˜
6. âœ… åˆ›å»ºç¤ºä¾‹ä»£ç  (example_usage.py) - å¼€ç®±å³ç”¨çš„ä¾‹å­

## é‡è¦é…ç½®è¯´æ˜

### auto_map æ˜¯ä»€ä¹ˆï¼Ÿ

`auto_map` æ˜¯ HuggingFace çš„ä¸€ä¸ªæœºåˆ¶ï¼Œè®© `AutoModel.from_pretrained()` çŸ¥é“å»å“ªé‡Œæ‰¾ä½ çš„æ¨¡å‹ç±»ã€‚

è„šæœ¬ä¼šè‡ªåŠ¨åœ¨ `config.json` ä¸­æ·»åŠ ï¼š

```json
{
  "auto_map": {
    "AutoConfig": "configuration_llava_vit.LlavaViTConfig",
    "AutoModel": "modeling_llava_vit.LlavaViTModel"
  }
}
```

### trust_remote_code æ˜¯ä»€ä¹ˆï¼Ÿ

å› ä¸ºä½ çš„æ¨¡å‹ä»£ç ä¸åœ¨ transformers åº“é‡Œï¼Œè€Œæ˜¯åœ¨ä½ çš„ HuggingFace ä»“åº“ä¸­ï¼Œæ‰€ä»¥ç”¨æˆ·åŠ è½½æ—¶éœ€è¦ï¼š

```python
model = AutoModel.from_pretrained(
    "your-repo", 
    trust_remote_code=True  # å¿…é¡»è®¾ç½®ï¼
)
```

è¿™å‘Šè¯‰ transformersï¼š"æˆ‘ä¿¡ä»»è¿™ä¸ªä»“åº“çš„ä»£ç ï¼Œå¯ä»¥æ‰§è¡Œå®ƒ"ã€‚

## å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆéœ€è¦ä¸Šä¼ ä¸¤ä¸ª Python æ–‡ä»¶ï¼Ÿ

A: 
- `configuration_llava_vit.py` - å®šä¹‰æ¨¡å‹é…ç½®
- `modeling_llava_vit.py` - å®šä¹‰æ¨¡å‹ç»“æ„

è¿™æ˜¯ HuggingFace çš„æ ‡å‡†åšæ³•ï¼Œè®©ä½ çš„æ¨¡å‹å¯ä»¥è¢« AutoModel è¯†åˆ«ã€‚

### Q2: æƒé‡æ–‡ä»¶æ€ä¹ˆå‡†å¤‡ï¼Ÿ

A: ä½ çš„ checkpoint.pth åº”è¯¥æ˜¯ä¸€ä¸ªåŒ…å«æ¨¡å‹æƒé‡çš„æ–‡ä»¶ï¼Œé€šå¸¸æ˜¯ï¼š

```python
# ä¿å­˜æ—¶
torch.save({
    'model': model.state_dict(),
    'epoch': epoch,
    # ... å…¶ä»–ä¿¡æ¯
}, 'checkpoint.pth')

# æˆ–è€…ç›´æ¥ä¿å­˜
torch.save(model.state_dict(), 'checkpoint.pth')
```

### Q3: å¯ä»¥ä¸æä¾›æƒé‡å—ï¼Ÿ

A: å¯ä»¥ï¼ä¸æä¾› `--weight_path` æ—¶ï¼Œä¼šä¸Šä¼ éšæœºåˆå§‹åŒ–çš„æ¨¡å‹ã€‚è¿™å¯¹äºï¼š
- æµ‹è¯•ä¸Šä¼ æµç¨‹
- åˆ†äº«æ¨¡å‹æ¶æ„
- åç»­å†æ›´æ–°æƒé‡

éƒ½å¾ˆæœ‰ç”¨ã€‚

### Q4: å¦‚ä½•æ›´æ–°å·²ä¸Šä¼ çš„æ¨¡å‹ï¼Ÿ

A: ç›´æ¥ç”¨ç›¸åŒçš„ `--repo_id` å†æ¬¡è¿è¡Œè„šæœ¬ï¼Œä¼šè¦†ç›–ä¹‹å‰çš„ç‰ˆæœ¬ï¼š

```bash
python model_factory/upload_llava_vit_to_hf.py \
    --model_name hf_llava_vit_large_ln \
    --weight_path /path/to/new_checkpoint.pth \
    --repo_id your-username/llava-vit-large \
    --token YOUR_HF_TOKEN
```

### Q5: ç§æœ‰ä»“åº“æ€ä¹ˆåˆ›å»ºï¼Ÿ

A: æ·»åŠ  `--private` å‚æ•°ï¼š

```bash
python model_factory/upload_llava_vit_to_hf.py \
    --model_name hf_llava_vit_large_ln \
    --weight_path /path/to/checkpoint.pth \
    --repo_id your-username/private-model \
    --token YOUR_HF_TOKEN \
    --private
```

## å®Œæ•´å·¥ä½œæµ

### 1. è®­ç»ƒæ¨¡å‹

```python
import timm

# åˆ›å»ºå¹¶è®­ç»ƒä½ çš„æ¨¡å‹
model = timm.create_model('hf_llava_vit_large_ln', pretrained=False)
# ... è®­ç»ƒä»£ç  ...

# ä¿å­˜æƒé‡
torch.save({
    'model': model.state_dict(),
    'epoch': final_epoch,
}, 'trained_model.pth')
```

### 2. ä¸Šä¼ åˆ° HuggingFace

```bash
export HF_TOKEN=hf_your_token_here

python model_factory/upload_llava_vit_to_hf.py \
    --model_name hf_llava_vit_large_ln \
    --weight_path trained_model.pth \
    --repo_id your-username/my-awesome-vit
```

### 3. åœ¨å…¶ä»–åœ°æ–¹ä½¿ç”¨

```python
from transformers import AutoModel, CLIPImageProcessor
import torch
from PIL import Image

# åŠ è½½æ¨¡å‹
model = AutoModel.from_pretrained(
    "your-username/my-awesome-vit",
    trust_remote_code=True
)
processor = CLIPImageProcessor.from_pretrained("your-username/my-awesome-vit")

# åŠ è½½å¹¶å¤„ç†å›¾ç‰‡
image = Image.open("your_image.jpg")
inputs = processor(images=image, return_tensors="pt")

# æ¨ç†
with torch.no_grad():
    outputs = model(**inputs)
    embeddings = outputs.last_hidden_state
    pooled = outputs.pooler_output

print(f"Got embeddings: {embeddings.shape}")
```

## é«˜çº§ç”¨æ³•

### ä½¿ç”¨è§†é¢‘è¾“å…¥

```python
# è§†é¢‘: [batch, channels, frames, height, width]
video = torch.randn(1, 3, 8, 448, 448)
outputs = model(pixel_values=video)
```

### ä½¿ç”¨ Maskingï¼ˆæé«˜æ•ˆç‡ï¼‰

```python
# åªå¤„ç†éƒ¨åˆ† patches
pixel_values = torch.randn(1, 3, 448, 448)
num_patches = (448 // 14) ** 2
visible_indices = torch.arange(num_patches // 2).unsqueeze(0)  # åªç”¨ä¸€åŠ

outputs = model(
    pixel_values=pixel_values,
    visible_indices=visible_indices
)
```

### æ‰¹é‡æ¨ç†

```python
# æ‰¹é‡å¤„ç†å¤šå¼ å›¾ç‰‡
images = [Image.open(f"image_{i}.jpg") for i in range(10)]
inputs = processor(images=images, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)
    # outputs.last_hidden_state çš„ batch_size å°†æ˜¯ 10
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. ä½¿ç”¨åŠç²¾åº¦

```python
model = AutoModel.from_pretrained(
    "your-repo",
    trust_remote_code=True,
    torch_dtype=torch.bfloat16  # æˆ– torch.float16
).cuda()
```

### 2. ä½¿ç”¨ Flash Attention

ç¡®ä¿å®‰è£…äº† `flash_attn`ï¼š

```bash
pip install flash-attn --no-build-isolation
```

### 3. æ‰¹é‡å¤„ç†

```python
# ä¸å¥½ï¼šé€ä¸ªå¤„ç†
for img in images:
    output = model(processor(img, return_tensors="pt"))

# å¥½ï¼šæ‰¹é‡å¤„ç†
inputs = processor(images=images, return_tensors="pt")
outputs = model(**inputs)
```

## æµ‹è¯•ä½ çš„ä¸Šä¼ 

ä½¿ç”¨æä¾›çš„æµ‹è¯•è„šæœ¬ï¼š

```bash
python model_factory/test_automodel_loading.py your-username/llava-vit-large
```

è¿™ä¼šè‡ªåŠ¨æµ‹è¯•ï¼š
- âœ… é…ç½®åŠ è½½
- âœ… æ¨¡å‹åŠ è½½
- âœ… å›¾åƒè¾“å…¥
- âœ… è§†é¢‘è¾“å…¥
- âœ… Masking åŠŸèƒ½

## éœ€è¦å¸®åŠ©ï¼Ÿ

1. æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£ï¼š`model_factory/README_UPLOAD_TO_HF.md`
2. æŸ¥çœ‹ç¤ºä¾‹ä»£ç ï¼šä¸Šä¼ åçš„ `example_usage.py`
3. æµ‹è¯•åŠ è½½ï¼š`test_automodel_loading.py`

## æ£€æŸ¥æ¸…å•

ä¸Šä¼ å‰ç¡®ä¿ï¼š

- [ ] å·²å®‰è£…ä¾èµ–ï¼š`pip install huggingface_hub transformers timm torch`
- [ ] å·²è·å– HF Tokenï¼šhttps://huggingface.co/settings/tokens
- [ ] æƒé‡æ–‡ä»¶è·¯å¾„æ­£ç¡®
- [ ] é€‰æ‹©äº†æ­£ç¡®çš„ model_name
- [ ] repo_id æ ¼å¼æ­£ç¡®ï¼š`username/model-name`

ä¸Šä¼ åéªŒè¯ï¼š

- [ ] è®¿é—® `https://huggingface.co/your-username/model-name` æ£€æŸ¥æ–‡ä»¶
- [ ] README.md æ˜¾ç¤ºæ­£å¸¸
- [ ] è¿è¡Œæµ‹è¯•è„šæœ¬éªŒè¯åŠ è½½
- [ ] å°è¯•å®é™…æ¨ç†

å®Œæˆï¼ğŸ‰
